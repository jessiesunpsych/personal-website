---
# Documentation: https://wowchemy.com/docs/managing-content/

title: '(Not) hearing happiness: Predicting fluctuations in happy mood from acoustic
  cues using machine learning'
subtitle: ''
summary: ''
authors:
- Aaron C. Weidman
- Jessie Sun
- Simine Vazire
- Jordi Quoidbach
- Lyle H. Ungar
- Elizabeth W. Dunn
tags:
- '"Acoustic analysis"'
- '"Experience-sampling"'
- '"Happiness"'
- '"Happy mood"'
- '"Machine learning"'
categories: []
date: '2019-01-01'
lastmod: 2021-10-05T20:39:10-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-10-06T00:39:10.542538Z'
publication_types:
- '2'
abstract: Recent popular claims surrounding virtual assistants suggest that computers
  will soon be able to hear our emotions. Supporting this possibility, promising work
  has harnessed big data and emergent technologies to automatically predict stable
  levels of one specific emotion, happiness, at the community (e.g., counties) and
  trait (i.e., people) levels. Furthermore, research in affective science has shown
  that non-verbal vocal bursts (e.g., sighs, gasps) and specific acoustic features
  (e.g., pitch, energy) can differentiate between distinct emotions (e.g., anger,
  happiness), and that machine-learning algorithms can detect these differences. Yet,
  to our knowledge, no work has tested whether computers can automatically detect
  normal, everyday within-person fluctuations in one emotional state from acoustic
  analysis. To address this issue in the context of happy mood, across three studies
  (total N = 20,197), we asked participants to repeatedly report their state happy
  mood, and to provide audio ecordings—including both direct speech and ambient sounds—from
  which we extracted acoustic features. Using three different machine learning algorithms
  (neural networks, random forests, and support vector machines) and two sets of acoustic
  features, we found that acoustic features yielded minimal predictive insight into
  happy mood above chance. Neither multilevel modeling analyses nor human coders provided
  additional insight into state happy mood. These findings suggest that it is not
  yet possible to automatically assess fluctuations in one emotional state (i.e.,
  happy mood) from acoustic analysis, pointing to a critical future direction for
  affective scientists interested in acoustic analysis of emotion and automated emotion
  detection.
publication: '*Emotion*'
doi: 10.1037/emo0000571
---
